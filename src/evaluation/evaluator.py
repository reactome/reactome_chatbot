import argparse
import os

import pandas as pd
from datasets import Dataset
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers.merger_retriever import MergerRetriever
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain_chroma.vectorstores import Chroma
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_community.retrievers import BM25Retriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from ragas import evaluate
from ragas.metrics import answer_relevancy, context_utilization, faithfulness, context_recall

from conversational_chain.chain import create_rag_chain
from reactome.metadata_info import descriptions_info, field_info


def parse_arguments():
    """Parse command line arguments for the script."""
    parser = argparse.ArgumentParser(
        description="Load a directory of testsets and evaluate answers generated by a language model."
    )
    parser.add_argument(
        "--testset_dir",
        type=str,
        required=True,
        help="Path to the directory containing testset Excel (.xlsx) files",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini",
        help="Language model to use for evaluation",
    )
    parser.add_argument(
        "--rag_type",
        choices=["basic", "advanced"],
        required=True,
        help="Type of RAG system to use for evaluation",
    )
    return parser.parse_args()


def load_dataset(testset_path):
    """Load the dataset from an Excel (.xlsx) file."""
    try:
        df = pd.read_excel(testset_path)
        return df.to_dict(
            orient="records"
        )  # Convert DataFrame to a list of dictionaries
    except FileNotFoundError:
        raise FileNotFoundError(f"The file {testset_path} does not exist.")
    except ValueError as e:
        raise ValueError(f"Error reading the Excel file: {e}")


memory = ChatHistoryMemory()


def initialize_rag_chain_with_memory(embeddings_directory, model_name, rag_type):
    """Initialize the RAGChainWithMemory system."""
    llm = ChatOpenAI(temperature=0.0, verbose=True, model=model_name)
    retriever_list = []

    loader = CSVLoader(
        "/Users/hmohammadi/Desktop/react_to_me_github/reactome_chatbot/embeddings/openai/text-embedding-3-large/reactome/summation_csv/summations.csv"
    )
    data = loader.load()
    bm25_retriever = BM25Retriever.from_documents(data)
    bm25_retriever.k = 7

    # Set up vectorstore SelfQuery retriever
    embedding = OpenAIEmbeddings(model="text-embedding-3-large")
    vectordb = Chroma(
        persist_directory=embeddings_directory,
        embedding_function=embedding,
    )

    vectordb_retriever = vectordb.as_retriever(search_kwargs={"k": 7})

    selfq_retriever = SelfQueryRetriever.from_llm(
        llm=llm,
        vectorstore=vectordb,
        document_contents=descriptions_info["summations"],
        metadata_field_info=field_info["summations"],
        search_kwargs={"k": 7},
    )
    rrf_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, selfq_retriever], weights=[0.2, 0.8]
    )
    if rag_type == "basic":
        retriever_list.append(vectordb_retriever)
    elif rag_type == "advanced":
        retriever_list.append(rrf_retriever)

    reactome_retriever = MergerRetriever(retrievers=retriever_list)

    qa = create_rag_chain(
        retriever=reactome_retriever,
        llm=llm,
    )
    return qa


def process_testset(
    testset_path, qa_system, embeddings_directory, response_dir, eval_dir, model_name, rag_type
):
    """Process a single testset file."""
    testset = load_dataset(testset_path)
    questions = [item["question"] for item in testset]
    ground_truths = [item["ground_truth"] for item in testset]

    answers = []
    contexts = []

    for question in questions:
        response = qa_system.get_context(question)
        answers.append(response["answer"])
        contexts.append([context.page_content for context in response["context"]])

    rag_response_dir = os.path.join(response_dir, rag_type)
    rag_eval_dir = os.path.join(eval_dir, rag_type)
    os.makedirs(rag_response_dir, exist_ok=True)
    os.makedirs(rag_eval_dir, exist_ok=True)
    
    # Save responses to an Excel file
    data = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths,
    }
    df_ans = pd.DataFrame(data)
    response_filename = os.path.join(
        rag_response_dir,
        f"{os.path.splitext(os.path.basename(testset_path))[0]}_{model_name}_responses_{rag_type}.xlsx",
    )
    df_ans.to_excel(response_filename, index=False)
    print(f"Responses saved to {response_filename}")

    # Evaluate the dataset
    dataset = Dataset.from_dict(data)
    result = evaluate(
        llm=ChatOpenAI(temperature=0.0, verbose=True, model="gpt-4o"),
        dataset=dataset,
        metrics=[answer_relevancy, context_utilization, faithfulness, context_recall],
    )

    # Save evaluation results to an Excel file
    evaluation_filename = os.path.join(
        rag_eval_dir,
        f"{os.path.splitext(os.path.basename(testset_path))[0]}_{model_name}_evaluation_{rag_type}.xlsx",
    )
    df_eval = result.to_pandas()
    df_eval.to_excel(evaluation_filename, index=False)
    print(f"Evaluation results saved to {evaluation_filename}")


def main():
    args = parse_arguments()
    model_name = args.model
    rag_type = args.rag_type
    response_dir = os.path.join(args.testset_dir, "response")
    eval_dir = os.path.join(args.testset_dir, "evals")
    os.makedirs(response_dir, exist_ok=True)
    os.makedirs(eval_dir, exist_ok=True)

    # Initialize RAG Chain
    embeddings_directory = "/Users/hmohammadi/Desktop/react_to_me_github/reactome_chatbot/embeddings/openai/text-embedding-3-large/reactome/Release90/summations"
    qa_system = initialize_rag_chain_with_memory(embeddings_directory, model_name, rag_type)

    # Iterate over all .xlsx files in the directory
    for filename in os.listdir(args.testset_dir):
        print(f"Found file: {filename}")
        if filename.endswith(".xlsx"):
            testset_path = os.path.join(args.testset_dir, filename)
            print(f"Processing testset: {testset_path}")
            process_testset(
                testset_path,
                qa_system,
                embeddings_directory,
                response_dir,
                eval_dir,
                model_name,
                rag_type,
            )


if __name__ == "__main__":
    main()
