import argparse
import json

from datasets import Dataset, load_dataset
from langchain_openai import ChatOpenAI
from ragas import evaluate

from src.retrieval_chain import initialize_retrieval_chain


def parse_arguments():
    """Parse command line arguments for the script."""
    parser = argparse.ArgumentParser(
        description="Load a test set and evaluate answers generated by a language model."
    )
    parser.add_argument(
        "--testset_path", type=str, required=True, help="Path to the testset JSON file"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-3.5-turbo-0125",
        help="Language model to use for evaluation",
    )
    parser.add_argument(
        "--metrics",
        type=str,
        nargs="+",
        help="List of evaluation metrics (e.g., precision, recall)",
    )
    parser.add_argument(
        "--replicates",
        type=int,
        default=1,
        help="Number of replicates for answer generation",
    )
    return parser.parse_args()


def load_dataset(testset_path):
    """Load the dataset from a JSON file."""
    try:
        with open(testset_path, "r") as file:
            data = json.load(file)
        return data
    except FileNotFoundError:
        raise FileNotFoundError(f"The file {testset_path} does not exist.")
    except json.JSONDecodeError:
        raise ValueError("Invalid JSON file format.")


def main():
    args = parse_arguments()

    # Load testset from JSON file
    testset = load_dataset(args.testset_path)

    questions = [item["question"] for item in testset]
    ground_truths = [item["ground_truth"] for item in testset]

    qa_system = initialize_retrieval_chain(embeddings_directory, True)

    # This will hold the final results for all replicates
    all_replicates_results = []

    for _ in range(args.replicates):
        answers = []
        contexts = []

        for question in questions:
            response = qa_system.invoke(question)
            answers.append(response["answer"])
            contexts.append(
                [context.page_content for context in response["source_documents"]]
            )

        data = {
            "question": questions,
            "answer": answers,
            "contexts": contexts,
            "ground_truth": ground_truths,
        }

        # Convert dict to a dataset object
        dataset = Dataset.from_dict(data)
        all_replicates_results.append(dataset)

    # Evaluation can be done outside the loop for each dataset or after aggregating results, depending on your needs
    for dataset in all_replicates_results:
        metrics_to_use = [
            eval(metric) for metric in args.metrics
        ]  # Assuming metrics are safely evaluable
        result = evaluate(
            llm=ChatOpenAI(temperature=0.0, verbose=True, model=args.model),
            dataset=dataset,
            metrics=metrics_to_use,
        )

        df = result.to_pandas()
        print(df)


if __name__ == "__main__":
    main()
